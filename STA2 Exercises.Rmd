---
title: "STA2 Exercises"
output:
  md_document:
    variant: markdown_github
---

# Green buildings

```{r}
library(mosaic)

greenbuildingsdata = read.csv('data/greenbuildings.csv')

data(greenbuildingsdata)
greenbuildingsdata
head(greenbuildingsdata)
summary(greenbuildingsdata)
```


```{r}
median_green=median(greenbuildingsdata$Rent[greenbuildingsdata$green_rating==1])  
median_nongreen=median(greenbuildingsdata$Rent[greenbuildingsdata$green_rating==0])
cat("Median rent for green buildings:", median_green, "\nMdeian rent for nongreen buildings:", median_nongreen)
```

Through his analysis, the Excel guru made a case that there is a premium upside when constructing green buildings. The Excel guru anticipated a $2.60 per square foot premium for constructing a green building by calculating the median values of non green buildings and green buildings separately. He implies a $650,000 in revenue for a $250,000 square foot building. These computations, however, are very superficial and my team performed further exploratory analysis to verify these findings.

```{r}
ggplot(data = greenbuildingsdata) + 
  geom_histogram(mapping = aes(x = Rent, y = stat(density)), binwidth = 2) +
  facet_grid(green_rating~.)
```
 
First, our team began by looking at the variability in the data. We created density plots using histograms and divided the plots between green buildings and non green buildings to analyze how we should filter out data that would skew our findings. Because we know that our building is new and would be 250,000 square feet, we would have to filter out and perform analysis on similar buildings.

```{r}
ggplot(data = greenbuildingsdata) + 
  geom_histogram(mapping = aes(x = age, y = stat(density)), binwidth = 2) +
  facet_grid(green_rating~.)

greenbuildings1 = mutate(greenbuildingsdata,
                        agecat = cut(age, c(-1, 01, 25, 50, 75, 200)))

sum1 = greenbuildings1 %>%
  group_by(agecat, green_rating) %>%
  summarize(mean_rent = mean(Rent), n = n())

sum1

ggplot(sum1) +
  geom_bar(stat='identity', aes(x=agecat, y=mean_rent, fill=factor(green_rating)), position = 'dodge')

# then use geom_bar
```
 
 The density for ages above 50 years old is significantly unbalanced. 
 
```{r}
options(scipen=999)

ggplot(data = greenbuildingsdata) + 
  geom_histogram(mapping = aes(x = size, y = stat(density))) +
  facet_grid(green_rating~.)

greenbuildings2 = mutate(greenbuildingsdata,
                         sizecat = cut(size, c(-1, 5000, 50000, 100000, 250000, 500000, 750000, 1000000, 2000000,4000000)))

sum2 = greenbuildings2 %>%
  group_by(sizecat, green_rating) %>%
  summarize(mean_rent = mean(Rent), n = n())

sum2

options(scipen=999)
require(scales)


df <- data.frame(x=seq(1, 1e9, length.out=100), y=sample(100))

point <- format_format(big.mark = " ", decimal.mark = ",", scientific = FALSE)

gg_size=ggplot(sum2) +
  geom_bar(stat='identity', aes(x=sizecat, y=mean_rent, fill=factor(green_rating)), position = 'dodge') +
  coord_flip() 
gg_size
```

Uneven distribution in very small and very large builings. 

```{r}
options(scipen=999)

ggplot(data = greenbuildingsdata) + 
  geom_histogram(mapping = aes(x = leasing_rate, y = stat(density))) +
  facet_grid(green_rating~.)

gg_leasingrate = mutate(greenbuildingsdata,
                        leasing_ratecat = cut(leasing_rate, c(-1, 01, 25, 50, 75, 100)))

sum4 = gg_leasingrate %>%
  group_by(leasing_ratecat, green_rating) %>%
  summarize(mean_rent = mean(Rent), n = n())

sum4

ggplot(sum4) +
  geom_bar(stat='identity', aes(x=leasing_ratecat, y=mean_rent, fill=factor(green_rating)), position = 'dodge')

```

High variability in buildings with low leasing rates. 



These visualizations show variability in size, age and leasing rates of buildings. It is not sensible to compare buildings of differing size, age, and occupancy rates, and thus, decided to only include buildings with the following features:
- Buildings less than 50 years old
- Buildings greater than 5000 square feet and less than 1,000,000 square feet
- Buildings with leasing rates of greater than 25%

```{r}
greenbuildings <- greenbuildingsdata[which(greenbuildingsdata$age < 50 & greenbuildingsdata$size > 5000 & greenbuildingsdata$size < 1000000 & greenbuildingsdata$leasing_rate>25), ]
summary(greenbuildings)
green=greenbuildings[which(greenbuildings$green_rating ==1),]
nongreen=greenbuildings[which(greenbuildings$green_rating ==0),]
```


```{r}
median_filtereed_green=median(greenbuildings$Rent[greenbuildings$green_rating==1])
median_filtered_nongreen=median(greenbuildings$Rent[greenbuildings$green_rating==0])
cat("Median rent for filtered green buildings:", median_filtereed_green, "\nMedian rent for filtered non-green buildings:", median_filtered_nongreen)
```

We can see that the median rent per square foot in non-green buildings was $26.4 and the median rent per square foot for green buildings was $28. This shows that the more realistic deviation between green buildings and non-green buildings median rent is only $1.60 per square foot rather than the $2.60 that the Excel guru calculated. For a building that is greater than 250,000 square feet, that would translate into $400,000 of extra revenue per year instead of the $650,000 initial calculation.



Our next step was analyzing confounding variables that could help identify if there is a significant premium in rent for green buildings. We needed to explain if the $1.60 rent per square foot increase in green buildings was actually due to their green rating status or if there are other factors that may explain this price. (All the following scatter plots will be split by non green buildings (0) on the left, and green buildings (1) on the right.

```{r}
options(scipen=999)  # turn-off scientific notation like 1e+48

theme_set(theme_bw()) 

# Scatterplot
ggplot(greenbuildings, aes(x=size, y=Rent)) + 
  geom_point(aes(col=as.factor(amenities))) +
  geom_smooth(method="loess", se=F) +
  facet_grid(as.factor(amenities) ~ green_rating) +
  labs(subtitle="Size and Amenities vs Rent", 
       y="Rent", 
       x="Size", 
       title="Scatterplot", 
       caption = "Source: greenbuildings")

```

```{r}
median_filtered_green_amenities=median(green$Rent[green$amenities==1])
median_filtered_green_no_amenities=median(green$Rent[green$amenities==0])
median_filtered_nongreen_amenities=median(nongreen$Rent[nongreen$amenities==1])
median_filtered_nongreen_no_amenities=median(nongreen$Rent[nongreen$amenities==0])
cat("Median rent for filtered green buildings with amenities:", median_filtered_green_amenities, "\nMedian rent for filteered green buildings without amenities:", median_filtered_green_no_amenities, "\nMedian rent for filtered non green buildings with amenities:", median_filtered_nongreen_amenities, "\nMedian rent for filteered non-green buildings without amenities:", median_filtered_nongreen_no_amenities)

green_amenities=(green[which(green$amenities ==1),])
count(green_amenities)/count(green)
nongreen_amenities=(nongreen[which(nongreen$amenities ==1),])
count(nongreen_amenities)/count(nongreen)
```

Green buildings with amenities contribute nearly a $1.5 increase for rent per square foot over green buildings without amenities. Non-green buildings have a $2 rent per square foot increase if amenities are included or not.
73% of green buildings have amenities whereas only 60% of non green buildings have amenities. Thus, green buildings will have a higher median rent becuase they tend to have more amenities than non green buildings. 


Next we looked at how buildings in different cluster locations affect rent.

```{r}
ggplot(greenbuildings, aes(x=Rent, y=leasing_rate)) + 
  geom_point(aes(col=as.factor(class_a))) +
  geom_smooth(method="loess", se=F) +
  facet_grid(amenities ~ green_rating) +
  labs(subtitle="Rent vs Leasing Rate", 
       y="Leasing Rate", 
       x="Rent", 
       title="Scatterplot", 
       caption = "Source: greenbuildings")
```
Most green buildings are class A compared to non-green buildings. In addition, these buildings tend to have a higher leasing rates compared to non green buildings. This could inflate the value of green buildings as class A building are typically nicer. Lets dive more deeply into this. 


```{r}
median_green_a=median(green$Rent[green$class_a==1])
median_green_non_a=median(green$Rent[green$class_a==0])
median_nongreen_a=median(nongreen$Rent[nongreen$class_a==1])
median_nongreen_non_a=median(nongreen$Rent[nongreen$class_a==0])

cat("Median rent for class A green buildings:", median_green_a, "\nMedian rent for Class B green buildings:", median_green_non_a, "\nMedian rent for Class A non-green buildings", median_nongreen_a,  "\nMedian rent for Class B non-green buildings:", median_nongreen_non_a)

green_classa=(green[which(green$class_a ==1),])
count(green_classa)/count(green)
nongreen_classa=(nongreen[which(nongreen$class_a ==1),])
count(nongreen_classa)/count(nongreen)

```

As we can see here, the median rent Class A buildings in green buildings and non green buildings are very high compared to green buildings and non green buildings that are not Class A. Of green buildings, 81% are class A and only 52% of non green buildings are class A. This is definitely a reason why green buildings were caluclated to have a higher median rent than non green buildings. 


```{r}
options(scipen=999)

ggplot(data = greenbuildings) + 
  geom_histogram(mapping = aes(x = cluster, y = stat(density))) +
  facet_grid(green_rating~.)

# , binwidth = 20

greenbuildings5 = mutate(greenbuildings,
                        clustercat = cut(cluster, c(0, 250, 625, 875, 1250)))

sum5 = greenbuildings5 %>%
  group_by(clustercat, green_rating) %>%
  summarize(mean_cluster_rent = mean(cluster_rent), n = n())

sum5

ggplot(sum5) +
  geom_bar(stat='identity', aes(x=clustercat, y=mean_cluster_rent, fill=factor(green_rating)), position = 'dodge')
```

Clearly a buildingâ€™s rent is very dependent on its cluster location. Buildings in the 0- 250 cluster range pay significantly less in rent on average compared to buildings in the 200-1250 cluster range. However, across each cluster bins, both green buildings and non green buildings are relatively equal. 

```{r}
median_filtered_green_cluster1=median(green$Rent[green$cluster>=0 & green$cluster<250])
median_filtered_green_no_cluster2=median(green$Rent[green$cluster>=250 & green$cluster<625])
median_filtered_green_cluster3=median(green$Rent[green$cluster>=625 & green$cluster<1250])
median_filtered_nongreen_cluster1=median(nongreen$Rent[nongreen$cluster>=0 & nongreen$cluster<250])
median_filtered_nongreen_cluster2=mean(nongreen$Rent[nongreen$cluster>=250 & nongreen$cluster<625])
median_filtered_nongreen_cluster3=median(nongreen$Rent[nongreen$cluster>=625 & nongreen$cluster<1250])
cat("Median rent for filtered green buildings in clusters 0-250:", median_filtered_green_cluster1, "\nMedian rent for filteered green buildings in clusters 250-625:", median_filtered_green_no_cluster2, "\nMedian rent for filtered green buildings in clusters 625-1250:", median_filtered_green_cluster3, "\nMedian rent for filtered nongreen buildings in clusters 0-250:", median_filtered_nongreen_cluster1, "\nMedian rent for filtered nongreen buildings in clusters 250-625:", median_filtered_nongreen_cluster2, "\nMedian rent for filtered nongreen buildings in clusters 625-1250:", median_filtered_nongreen_cluster3)
```

```{r}
green_lowcluster=(green[which(green$cluster >=0 & green$cluster<250),])
count_green_low=nrow(green_lowcluster)/nrow(green)
nongreen_lowcluster=(nongreen[which(nongreen$cluster >=0 & nongreen$cluster<250),])
count_nongreen_low=nrow(nongreen_lowcluster)/nrow(nongreen)

cat("Percent of green buildings in low clusters:", count_green_low, "\nPercent of nongreen buildings in low:", count_nongreen_low)

```

There are more non green buildings in the lower clusters. This means that green buildings tend to be in clusters with high rent more so than non green buildings which would further inflate their median rent. To make sure that this is viable, however, we need to see if there are significantly more nongreen buildings with net contracts in these clusters. If so, then non green buildings may actually be paying more in rent + utilities than green buildings.

```{r}
ggplot(greenbuildings, aes(x=cluster, y=cluster_rent)) + 
  geom_point(aes(col=as.factor(net))) +
  geom_smooth(method="loess", se=F) +
  facet_grid(as.factor(net) ~ green_rating) +
  labs(subtitle="Cluster vs Cluster Rent Based on Net Contracts", 
       y="Cluster Rent", 
       x="Cluster", 
       title="Scatterplot", 
       caption = "Source: greenbuildings")

```

```{r}
green <- greenbuildings[ which(greenbuildings$green_rating==1), ]
nongreen <- greenbuildings[ which(greenbuildings$green_rating==0), ]

cat("Percent of nongreen buildings with net contracts:", nrow(nongreen[ which(nongreen$net==1), ])*100/nrow(nongreen), "\nPercent of green buildings with net contracts:", nrow(green[ which(green$net==1), ])*100/nrow(green))
```

The graph above shows that there are very few net contract buildings in both types of buildings. In fact, there are only 4.08% of non-green buildings with net-based contracts, and only 5.30% of green buildings with net based contracts. This difference is not substantial enough to account for the rent deviations. This confirms that the cluster location deviations in rent is not because of net contract based rent. Becuase there are more non green buildings in the lower clusters and that most buildings are not net based contracts, the median non green buildings rent is undervalued. Lets confirm that non green buildings are not paying higher in rent by looking at total degree days in both types of buildings.


```{r}
cat("Average number of total degree days for nongreen buildings:", mean(nongreen$total_dd_07), "\nAverage number of total degree days for green buildings:", mean(green$total_dd_07))
```

Our final research included the mean total amount of degree days in green buildings and non green buildings. Because both types of buildings have nearly the same number of total degree days, we can assume that the toal utility costs in both types of buildings are similar. Thus, green buildings and non green deviations are not caused by these deviations in rent, and so we can confidently say that the previous factors discussed are the major variables when determining rent differences between green buildings and non green buildings.  


```{r}
lin.fit=lm(Rent~size+leasing_rate+class_a+amenities+cluster+net+green_rating+green_rating*amenities+green_rating*class_a, data=greenbuildings)
summary(lin.fit)
```

In conclusion, investing in a green certification building is not a reasonable decision. There is no apparent premium for green buildings, when accounting for all possible confouding variables. 
 As explained using the visualizations, 73% of green buildings have amenities, 81% of them are class A buildings, and 83% of them are in clusters that pay higher rent which clearly increased the valuation of green buildings in the Excel Guru's initial calculations. Green ratings show to be insignificant when taking all other factors into account. Therefore we recommend the real estate developer should not invest in "going green". 

# Flights at ABIA

```{r}
library(ggplot2)
library(reshape2)
abia = read.csv("data/ABIA.csv")

#Creating variables
abia$dummy = 1

abia$carrier.dummy = ifelse(abia$CarrierDelay > 0, 1, 0)
abia$weather.dummy = ifelse(abia$WeatherDelay > 0, 1, 0)
abia$nas.dummy = ifelse(abia$NASDelay > 0, 1, 0)
abia$security.dummy = ifelse(abia$SecurityDelay > 0, 1, 0)
abia$lateaircraft.dummy = ifelse(abia$LateAircraftDelay > 0, 1, 0)

#Subsetting flights flying out of Austin
outbound_Aus = subset(abia, abia$Origin == "AUS")

#Subsetting flights flying out of Austin and have experienced delays
outbound_Aus_delays = subset(abia, abia$Origin == "AUS" & abia$DepDelay > 0)
```

####################################################################
## By Airline - Flying OUT OF Austin
####################################################################
```{r}
#Airlines delays
airline.delay = aggregate(outbound_Aus_delays$dummy,by=list(outbound_Aus_delays$UniqueCarrier),FUN = sum, na.rm=TRUE)
names(airline.delay)[1] = "UniqueCarrier"
names(airline.delay)[2] = "AnnualDelays"

#Airlines total outbound flights
airline.total = aggregate(outbound_Aus$dummy,by=list(outbound_Aus$UniqueCarrier),FUN = sum, na.rm=TRUE)
names(airline.total)[1] = "UniqueCarrier"
names(airline.total)[2] = "AnnualFlights"

# Calculating percentages
airline.delay.perc = merge(airline.total,airline.delay, by = "UniqueCarrier")
airline.delay.perc$per.delays = round(airline.delay.perc$AnnualDelays/airline.delay.perc$AnnualFlights,3)*100
```

This figure illustrates the total flights out of Austin by Airline.
```{r}
ggplot(data=airline.total, aes(reorder(x= UniqueCarrier, AnnualFlights), y= AnnualFlights)) + 
  geom_bar( stat = 'identity') + 
  ggtitle("Total # of Flights out of AUS by Airline") + 
  theme(plot.title = element_text(lineheight=1.2, face="bold")) + 
  xlab("Airlines") + 
  ylab("Number of flights") + 
  coord_flip()

```

This figure illustrates the total flights that are delayed leaving Austin Airport.
```{r}
ggplot(data=airline.delay, aes(reorder(x= UniqueCarrier, AnnualDelays), y= AnnualDelays)) + 
  geom_bar( stat = 'identity') + 
  # scale_size_area() + 
  ggtitle("Total # of Flights out of AUS Delayed by Airline") + 
  # theme(plot.title = element_text(lineheight=1.2, face="bold")) + 
  xlab("Airlines") + 
  ylab("Number of flights delayed") + 
  coord_flip()

```

So, Southwest Airlines has the highest number of flights and the highest number of flights delayed. Normalizing the variables, does it still have the highest delayed flight to total flight ratio? Why yes it does! Don't fly Southwest. 

To interpret the figure below, keep in mind that the average delay rate per airline is 30.23%. 
```{r}
library(tidyverse)

data(airline.delay.perc)

per.delays.scale = scale(airline.delay.perc$per.delays, center=TRUE, scale=FALSE)

per.delays.type = ifelse(per.delays.scale < 0, "below", "above")  # above / below avg flag

ggplot(data=airline.delay.perc, aes(reorder(x= UniqueCarrier, per.delays.scale), y = per.delays.scale)) + 
  geom_bar(stat = 'identity', aes(fill=per.delays.type, width=.5)) + 
  scale_fill_manual(name="Delays", 
                    labels = c("Above Average (Bad)", "Below Average(Good)"), 
                    values = c("above"="#f8766d", "below"="#00ba38")) +
  ggtitle("% of Flights delayed by Airline") + 
  xlab("Airlines") + 
  ylab("deviation from the mean of % flights delayed") + 
  coord_flip()

```

Aggregating Weather Delays by Airline
```{r}
airline.delay.types = aggregate(list(outbound_Aus_delays$carrier.dummy, outbound_Aus_delays$weather.dummy, outbound_Aus_delays$nas.dummy, outbound_Aus_delays$security.dummy, outbound_Aus_delays$lateaircraft.dummy),by=list(outbound_Aus_delays$UniqueCarrier),FUN = sum, na.rm=TRUE)
names(airline.delay.types)[1] = "UniqueCarrier"
names(airline.delay.types)[2] = "CarrierDelays"
names(airline.delay.types)[3] = "WeatherDelays"
names(airline.delay.types)[4] = "NASDelays"
names(airline.delay.types)[5] = "SecurityDelays"
names(airline.delay.types)[6] = "LateAircraftDelays"

airline.delay.types.perc = merge(airline.delay,airline.delay.types, by = "UniqueCarrier")

airline.delay.types.perc$per.delays.total = 100.0

airline.delay.types.perc$totalDelays = airline.delay.types.perc$CarrierDelays + airline.delay.types.perc$WeatherDelays + airline.delay.types.perc$NASDelays + airline.delay.types.perc$SecurityDelays + airline.delay.types.perc$LateAircraftDelays

airline.delay.types.perc$per.delays.carrier = round(airline.delay.types.perc$CarrierDelays/airline.delay.types.perc$totalDelays,3)*100

airline.delay.types.perc$per.delays.weather = round(airline.delay.types.perc$WeatherDelays/airline.delay.types.perc$totalDelays,3)*100

airline.delay.types.perc$per.delays.nas = round(airline.delay.types.perc$NASDelays/airline.delay.types.perc$totalDelays,3)*100

airline.delay.types.perc$per.delays.security = round(airline.delay.types.perc$SecurityDelays/airline.delay.types.perc$totalDelays,3)*100

airline.delay.types.perc$per.delays.late = round(airline.delay.types.perc$LateAircraftDelays/airline.delay.types.perc$totalDelays,3)*100
```


Now that we know which Airlines have the most delays on average, we can see why their flights are delayed. We accomplish this by identifying the percentage of reasons that flights were delayed by Airline.

```{r}
airline.delay.types.perc.selct <- airline.delay.types.perc[airline.delay.types.perc$UniqueCarrier %in% c("WN", "EV", "DL", "XE", "OH", "B6", "NW"), ]

airline.delay.types.perc.selct <- airline.delay.types.perc.selct[c(1, 10:14)]

airline.delay.types.perc.selct2 <- melt(airline.delay.types.perc.selct, id.vars='UniqueCarrier')

ggplot(airline.delay.types.perc.selct2, aes(x=UniqueCarrier, y=value, fill=variable)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Most Delayed Airlines", 
       subtitle="% of Delay Reasons", 
       x="Airline",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
  
```

And we can compare these percentages to the least delayed airlines.
```{r}
airline.delay.types.perc.selct <- airline.delay.types.perc[airline.delay.types.perc$UniqueCarrier %in% c("CO", "MQ", "OO", "AA", "UA", "F9", "YV", "9E", "US"), ]

airline.delay.types.perc.selct <- airline.delay.types.perc.selct[c(1, 10:14)]

airline.delay.types.perc.selct2 <- melt(airline.delay.types.perc.selct, id.vars='UniqueCarrier')

ggplot(airline.delay.types.perc.selct2, aes(x=UniqueCarrier, y=value, fill=variable)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Least Delayed Airlines", 
       subtitle="% of Delay Reasons", 
       x="Airline",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

While the reasons differ per airline, we can check whether the top 3 airlines on average are delayed for different reasons than the bottom 3 airlines.

```{r}
theme_set(theme_classic())

airline.delay.types.perc.select.test <- airline.delay.types.perc[airline.delay.types.perc$UniqueCarrier %in% c("WN", "EV", "DL"), ]

airline.delay.types.perc.select.test <- airline.delay.types.perc.select.test[c(1, 10:14)]
airline.delay.types.perc.select.test = airline.delay.types.perc.select.test[,c(2:6)]
bottom3_airlines = colMeans(airline.delay.types.perc.select.test)
bottom3_airlines = data.frame(bottom3_airlines)


airline.delay.types.perc.select.test2 <- airline.delay.types.perc[airline.delay.types.perc$UniqueCarrier %in% c( "YV", "9E", "US"), ]

airline.delay.types.perc.select.test2 <- airline.delay.types.perc.select.test2[c(1, 10:14)]
airline.delay.types.perc.select.test2 = airline.delay.types.perc.select.test2[,c(2:6)]
top3_airlines = colMeans(airline.delay.types.perc.select.test2)
top3_airlines = data.frame(top3_airlines)

airline_ratings = data.frame()

airline_grp_perf = cbind(top3_airlines, bottom3_airlines)
airline_grp_perf = t(airline_grp_perf)

airline_ratings <- melt(airline_grp_perf)

ggplot(airline_ratings, aes(x=Var1, y=value, fill=Var2)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Top and bottom 3 Airlines", 
       subtitle="% of Delay Reasons", 
       x="Airlines (top = the least delayed airlines)",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```
We can see that the most delayed airlines tend to be more commonly delayed due to reasons they cannot control (Weather and NAS) than the lest delayed airlines.


####################################################################
## Destinations - Flying OUT OF Austin
####################################################################

```{r}
#Destinations delay count
dest.delay = aggregate(outbound_Aus_delays$dummy,by=list(outbound_Aus_delays$Dest),FUN = sum, na.rm=TRUE)
names(dest.delay)[1] = "Destination"
names(dest.delay)[2] = "AnnualDelays"

#Destinations total outbound count
dest.total = aggregate(outbound_Aus$dummy,by=list(outbound_Aus$Dest),FUN = sum, na.rm=TRUE)
names(dest.total)[1] = "Destination"
names(dest.total)[2] = "AnnualFlights"

dest.delay.perc = merge(dest.total,dest.delay, by = "Destination")
dest.delay.perc$per.delays =round(dest.delay.perc$AnnualDelays/dest.delay.perc$AnnualFlights,3)*100
```

This figure plots the % of flights delayed by location. Almost 100% of flights to Des Moines, Iowa are delayed.
```{r}
ggplot(data=dest.delay.perc, aes(reorder(x= Destination, per.delays), y= per.delays)) + 
  geom_bar(stat = 'identity') + 
  ggtitle("% of Flights delayed by Destination") + 
  xlab("Destinations") + 
  ylab("% of flights delayed") + 
  coord_flip()
```

There are a lot of different destinations in the above graph. So, to make the charts more interpretable, how about we just look solely at popular destinations?
```{r}
ggplot(data=subset(dest.delay.perc,dest.delay.perc$AnnualFlights> median(dest.delay.perc$AnnualFlights)), aes(reorder(x= Destination, per.delays), y= per.delays)) + 
  geom_bar(stat = 'identity') + 
  ggtitle("% of Flights delayed by Popular Destinations") + 
  xlab("Destinations") + 
  ylab("% of flights delayed") + 
  coord_flip()
```

That was still hard to interpret. Let's look at the percentage differnce from the average, which is 36.66% of flights delayed (by destination). 
```{r}
data(dest.delay.perc)

dest.delay.perc.subset = subset(dest.delay.perc,dest.delay.perc$AnnualFlights> median(dest.delay.perc$AnnualFlights))

data(dest.delay.perc.subset)

average_delays_by_dest = mean(dest.delay.perc.subset$per.delays)
print(average_delays_by_dest)

dest.delay.perc.subset$dest.delay.scale = scale(dest.delay.perc.subset$per.delays, center=TRUE, scale=FALSE)

dest.delay.perc.subset$dest.delay.type = ifelse(dest.delay.perc.subset$dest.delay.scale < 0, "below", "above")  # above / below avg flag

ggplot(data=dest.delay.perc.subset, aes(reorder(x= dest.delay.perc.subset$Destination, dest.delay.perc.subset$dest.delay.scale), y= dest.delay.perc.subset$dest.delay.scale)) + 
  geom_bar(stat = 'identity', aes(fill=dest.delay.perc.subset$dest.delay.type, width=.5)) + 
  scale_fill_manual(name="Delays", 
                    labels = c("Above Average (Bad)", "Below Average (Good)"), 
                    values = c("above"="#f8766d", "below"="#00ba38")) +
  ggtitle("% of Flights delayed by Popular Destinations") + 
  xlab("Destination") + 
  ylab("deviation from the mean of % flights delayed") + 
  coord_flip()
```

Aggregating Weather Delays by Destination OUT OF Austin
```{r}
dest.delay.types = aggregate(list(outbound_Aus_delays$carrier.dummy, outbound_Aus_delays$weather.dummy, outbound_Aus_delays$nas.dummy, outbound_Aus_delays$security.dummy, outbound_Aus_delays$lateaircraft.dummy),by=list(outbound_Aus_delays$Dest),FUN = sum, na.rm=TRUE)
names(dest.delay.types)[1] = "Destination"
names(dest.delay.types)[2] = "CarrierDelays"
names(dest.delay.types)[3] = "WeatherDelays"
names(dest.delay.types)[4] = "NASDelays"
names(dest.delay.types)[5] = "SecurityDelays"
names(dest.delay.types)[6] = "LateAircraftDelays"

dest.delay.types.perc = merge(dest.delay,dest.delay.types, by = "Destination")

dest.delay.types.perc$per.delays.total = 100.0

dest.delay.types.perc$totalDelays = dest.delay.types.perc$CarrierDelays + dest.delay.types.perc$WeatherDelays + dest.delay.types.perc$NASDelays + dest.delay.types.perc$SecurityDelays + dest.delay.types.perc$LateAircraftDelays

dest.delay.types.perc$per.delays.carrier = round(dest.delay.types.perc$CarrierDelays/dest.delay.types.perc$totalDelays,3)*100

dest.delay.types.perc$per.delays.weather = round(dest.delay.types.perc$WeatherDelays/dest.delay.types.perc$totalDelays,3)*100

dest.delay.types.perc$per.delays.nas = round(dest.delay.types.perc$NASDelays/dest.delay.types.perc$totalDelays,3)*100

dest.delay.types.perc$per.delays.security = round(dest.delay.types.perc$SecurityDelays/dest.delay.types.perc$totalDelays,3)*100

dest.delay.types.perc$per.delays.late = round(dest.delay.types.perc$LateAircraftDelays/dest.delay.types.perc$totalDelays,3)*100

```

In this figure below we plot the most popular destinations that experience the most delays in the most popular destinations.
```{r}
dest.delay.types.perc.select <- dest.delay.types.perc[dest.delay.types.perc$Destination %in% c("SAN", "BWI", "ELP", "MDW", "HOU", "LAS", "BNA", "LBB", "EWR", "DAL", "MCO", "DEN", "JFK"), ]

dest.delay.types.perc.select <- dest.delay.types.perc.select[c(1, 10:14)]

dest.delay.types.perc.select2 <- melt(dest.delay.types.perc.select, id.vars='Destination')

ggplot(dest.delay.types.perc.select2, aes(x=Destination, y=value, fill=variable)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Top Delayed Destinations", 
       subtitle="% of Delay Reasons", 
       x="Destination",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

We can compare these reasons to the least delayed destinations in popular flights.
```{r}
dest.delay.types.perc.select <- dest.delay.types.perc[dest.delay.types.perc$Destination %in% c("PHX", "IAD", "ATL", "ORD", "CVG", "DFW", "IAH", "SFO", "SJC", "MEM", "CLT"), ]

dest.delay.types.perc.select <- dest.delay.types.perc.select[c(1, 10:14)]

dest.delay.types.perc.select2 <- melt(dest.delay.types.perc.select, id.vars='Destination')

ggplot(dest.delay.types.perc.select2, aes(x=Destination, y=value, fill=variable)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Least Delayed Destinations", 
       subtitle="% of Delay Reasons", 
       x="Destination",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

The delay reasons for popular destinations differ a lot; however, we will try to detect systematically different reasons between the three most delayed destinations and the three least delayed destinations.

```{r}
theme_set(theme_classic())

dest.delay.types.perc.select.test <- dest.delay.types.perc[dest.delay.types.perc$Destination %in% c("SAN", "BWI", "ELP"), ]

dest.delay.types.perc.select.test <- dest.delay.types.perc.select.test[c(1, 10:14)]
dest.delay.types.perc.select.test = dest.delay.types.perc.select.test[,c(2:6)]
bottom3_destinations = colMeans(dest.delay.types.perc.select.test)
bottom3_destinations = data.frame(bottom3_destinations)


dest.delay.types.perc.select.test2 <- dest.delay.types.perc[dest.delay.types.perc$Destination %in% c("SJC", "MEM", "CLT"), ]

dest.delay.types.perc.select.test2 <- dest.delay.types.perc.select.test2[c(1, 10:14)]
dest.delay.types.perc.select.test2 = dest.delay.types.perc.select.test2[,c(2:6)]
top3_destinations = colMeans(dest.delay.types.perc.select.test2)
top3_destinations = data.frame(top3_destinations)

destination_ratings = data.frame()

dest_grp_perf = cbind(top3_destinations, bottom3_destinations)
dest_grp_perf = t(dest_grp_perf)

destination_ratings <- melt(dest_grp_perf)

ggplot(destination_ratings, aes(x=Var1, y=value, fill=Var2)) +
  geom_bar(stat='identity', position = position_dodge(width = 0.8), width=0.8) +
  labs(title="Top and bottom 3 Destinations", 
       subtitle="% of Delay Reasons", 
       x="Destinations (top = flights to the least delayed destinations)",
       y="% of Reason Delayed",
       caption="source: ABIA") + 
  scale_fill_discrete(name = "Delayed Reason", labels = c("Carrier", "Weather", "NAS", "Security", "Late Arrival")) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```
Popular destinations with the worst delays tend to be delayed more due to late arrival of the preceeding flight than the popular destinations with the fewest delays.


# Portfolio modeling

We have chosen three separate portfolios. One consisting of 5 bond ETFs, to represent a very safe, low risk investment option. This means that the maximum loss as well as the VaR should be lower than our other two portfolios, but the average return should be fairly low, too.
The second is a diversified portfolio of 10 ETFs, including equities, commodities, bonds, currencies, and real estate. We expect this portfolio to have a higher average return than the first, but also a higher maximum loss and higher VaR.
Our third portfolio is a very risky portfolio. It includes 3 leveraged equity ETFs. This portfolio is expected to have the highest average return, but also the most risk, meaning that it will have a high maximum loss and a high VaR.

```{r}
library(ggplot2)
library(mosaic)
library(quantmod)
library(foreach)

myBondPortfolio = c("GOVT", "PZT", "VCSH", "VMBS", "LQD")
myDiversePortfolio = c("TIP", "EUFX", "LD", "IDU", "XLK", "FRI", "MNA", "XHB", "PPLT", "CORN")
myRiskyPortfolio = c("NUGT", "JNUG", "TECL")
getSymbols(c(myBondPortfolio,myDiversePortfolio,myRiskyPortfolio), from = "2014-08-08", to = "2019-08-09")

for(ticker in c(myBondPortfolio, myDiversePortfolio, myRiskyPortfolio)) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}

all_returns = cbind(ClCl(GOVTa),
                    ClCl(PZTa),
                    ClCl(VCSHa),
                    ClCl(VMBSa),
                    ClCl(LQDa),
                    ClCl(TIPa),
                    ClCl(EUFXa),
                    ClCl(LDa),
                    ClCl(IDUa),
                    ClCl(XLKa),
                    ClCl(FRIa),
                    ClCl(MNAa),
                    ClCl(XHBa),
                    ClCl(PPLTa),
                    ClCl(CORNa),
                    ClCl(NUGTa),
                    ClCl(JNUGa),
                    ClCl(TECLa))

all_returns = as.matrix(na.omit(all_returns))

initial_wealth = 100000
n_days = 20

set.seed(1) # make results reproducable

sim1 = foreach(i=1:10000, .combine='rbind') %do% {
	bond_wealth = initial_wealth
	diverse_wealth = initial_wealth
	risky_wealth = initial_wealth
	
	bond_weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
	diverse_weights = rep(0.1, 10)
	risky_weights = rep(1/3, 3)
	
	bond_holdings = bond_weights * bond_wealth
	diverse_holdings = diverse_weights * diverse_wealth
	risky_holdings = risky_weights * risky_wealth
	
	bond_wealthtracker = rep(0, n_days)
	diverse_wealthtracker = rep(0, n_days)
	risky_wealthtracker = rep(0, n_days)
	
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		bond_holdings = bond_holdings + bond_holdings*return.today[1:5]
		diverse_holdings = diverse_holdings + diverse_holdings*return.today[6:15]
		risky_holdings = risky_holdings + risky_holdings*return.today[16:18]
		
		bond_wealthtracker[today] = sum(bond_holdings)
	  diverse_wealthtracker[today] = sum(diverse_holdings)
	  risky_wealthtracker[today] = sum(risky_holdings)
	}
	c(bond_wealthtracker, diverse_wealthtracker, risky_wealthtracker)
}

bond_final = sim1[,n_days] - initial_wealth
diverse_final = sim1[,(2*n_days)] - initial_wealth
risky_final = sim1[,(3*n_days)] - initial_wealth

names = c("Bond Portfolio", "Diverse Portfolio", "Risky Portfolio")
means = c("Mean", mean(bond_final), mean(diverse_final), mean(risky_final))
best = c("Best performance", max(bond_final), max(diverse_final), max(risky_final))
worst = c("Worst performance", min(bond_final), min(diverse_final), min(risky_final))
var5 = c("VaR at 5% level", quantile(bond_final, 0.05), quantile(diverse_final, 0.05), quantile(risky_final, 0.05))

results = data.frame(" " = 0, "Bond Portfolio" = 0, "Diverse Portfolio" = 0, "Risky Portfolio" = 0)
results[1,]=means
results = rbind(results, best, worst, var5)

for (i in 1:4)
{
  for (j in 2:4)
    results[i, j] = round(as.numeric(results[i,j]),2)
}

results
```

As we can see from the table above, our expectations were correct. The bond portfolio has a low expected average return of \$279 after 4 weeks of trading, compared to \$859 for the diverse portfolio and \$10,139 for the risky portfolio.
The best performances also vary widely. The bond portfolio's best performance was \$3,445, compared to \$59,010 for the diverse portfolio and a gain of almost \$5.2 million for the risky portfolio! However, this increased return comes at a price, namely risk.
While the increased magnitude of worst performances give some indication, a better measure is value at risk. In 5% of the cases, we lose \$37,729 or more with our risky portfolio, compared to only \$4,763 for our diverse portfolio and $942 for our bond portfolio.

```{r}
portfolios = cbind(bond_final, diverse_final, risky_final)
ggplot() + 
  geom_histogram(aes(bond_final), fill = "green", binwidth = 1000, alpha = 0.5) +
  geom_histogram(aes(diverse_final[diverse_final <= 70000]), fill = "blue", binwidth = 1000, alpha = 0.5) + 
  geom_histogram(aes(risky_final[risky_final <= 70000]), fill = "red", binwidth = 1000, alpha = 0.5) +
  geom_smooth()
```
This plot does not include the total distribution for the diverse portfolio, since it has some extreme outliers (such as the $5.2 million profit simulation).
As seen in the plot above, the bond portfolios returns are all fairly close to 0 when compared to the other portfolios. As risk increases, the portfolios' final values start spreading a lot further from 0 (while the average slightly increases). For the risky portfolio, the tail to the right is extremely long.






# Market segmentation

Initially we will look at our data and try to discern if any initial data cleaning is necessary or if there are any gross outliers. We will do this by checking how many tweets are in each category, and looking at the dispersion of individuals' tweet counts. We will also check the correlation of variables.

```{r Initial Data Analysis}
library(ggplot2)
library(corrplot)

socialmarketing_raw = read.csv("data/social_marketing.csv")
socialmarketing = socialmarketing_raw[,-(1)]
attach(socialmarketing)

tag_frequency = colSums(socialmarketing)

tag_frequency = data.frame('names' = colnames(socialmarketing), 'frequency' = colSums((socialmarketing)))
tag_frequency = tag_frequency[order(-tag_frequency$frequency),]
tag_frequency$names = factor(tag_frequency$names, levels = tag_frequency$names)

ggplot(tag_frequency, aes(x=names, y=frequency)) +
  geom_bar(stat='identity',width=.5, fill="tomato3") + 
  labs(title = "Count of tweets in each category") +
  theme(axis.text.x = element_text(angle=90,vjust=0.5))

boxplot(rowSums(socialmarketing), main = "Tweets per user")
corrplot(cor(socialmarketing), order = "hclust")
```

The first plot illustrates that while there are extreme values, such as the chatter category, or users who tweeted over 100 times during this period, they are not necessarily outliers. 

However, as the problem states, chatter and uncategorized might have been used interchangably between different annotators. Thus, we will combine them into one variable. Furthermore, from the above correlation matrix we can discern the following strong relationships:
- cooking, fashion, beauty
- outdoors, health_nutrition, personal_fitness
- computers, travel, politics, news, automotive
- photo_sharing, chatter, shopping
- sports_playing, online_gaming, college_uni
- tv_film, art
- family, school, food, sports_fandom, religion, parenting

Given how many dimensions we currently have, we will be grouping these variables together, in order to reduce dimensionality. Furthermore, these groupings make sense when you think about them.


```{r Editing data frame}
socialmarketing1 = socialmarketing

socialmarketing1$chatter = chatter + uncategorized
socialmarketing1["cook_fash_beaut"] = cooking +fashion + beauty
socialmarketing1["out_health_fit"] = outdoors + health_nutrition + personal_fitness
socialmarketing1["comp_trav_pol_new_auto"] = computers + travel + politics + news + automotive
socialmarketing1["phot_chat_shop"] = photo_sharing + chatter + shopping 
socialmarketing1["sport_online_college"] = sports_playing + online_gaming + college_uni
socialmarketing1["tv_art"] = tv_film + art
socialmarketing1["fam"] = family + school + food + sports_fandom + religion + parenting

socialmarketing1 = subset(socialmarketing1, select= -c(cooking, fashion, beauty, outdoors, health_nutrition, personal_fitness, computers, travel, politics, photo_sharing, chatter, shopping, sports_playing, online_gaming, college_uni, tv_film, art, news, automotive, family, school, food, sports_fandom, religion, parenting, uncategorized))
```

Next, we will standardize the amount of tweets a certain user has in each category and then scale and center the data for each column.

```{r}
Z1 = socialmarketing1/rowSums(socialmarketing1)
X1 = scale(Z1, center=TRUE, scale=TRUE)
```

Now that we have our data cleaned, we will initially run a clustering algorithm to group users who talk about the same things. We will then identify what they tweet about most often and label them as such. This labeling will be useful for understanding our market segmentation later. The first step in doing this, is choosing the correct number of clusters.

```{r find k}
#KKN and CH
set.seed(10)
N = nrow(X1)
k_grid = c(1:30)
ss = c()
CH_grid = c()

for(k in k_grid) {
  cluster_k = kmeans(X1, k, nstart=50)
  W = cluster_k$tot.withinss
  ss = c(ss,W)
  
  B = cluster_k$betweenss
  CH = (B/W) * ((N - k) / (k - 1))
  CH_grid = c(CH_grid, CH)
}

plot(k_grid,ss)
plot(k_grid,CH_grid)
```

The elbow plot does not give us a clear k to use; however, the CH index suggests a k of 10, so we will use that in our algorithm.

```{r k-means}
set.seed(10)
kclust = kmeans(X1, 10, nstart=50)

for(i in 1:10)
  print(sort(kclust$centers[i,] , decreasing = TRUE)[1:4])

firstk = kclust$cluster
```

Above, the four variables with the highest value per cluster are displayed. From them we can discern a general trend of what these users tweet about most. The ten identified clusters are as follows:
(1) Adult Content
(2) TV and Art
(3) Dating
(4) Music
(5) College Athletes -- these individuals seem to be college athletes, as they talk most about college, playing sports and video games
(6) Beauty
(7) General Fitness
(8) Family
(9) Photo
(10) News

Now that we have a label that each user falls under, we will attempt to define market segments and analyze which users fall into which market segment.

```{r}
summary(Z1)
```

As we can see in the summary statistics of the scaled values above, no user falls into just one category. Thus, as there are no clear seperatable "buckets" that a user will fall into, we believe that running a PCA analysis will help us in more generally grouping these subclusters together.  Additionally, reduced dimensionality will allow for a clustering algorithm to perform better, since distance based algorithms tend to suffer from high dimensionality (curse of dimensionality).

```{r PCA 1}
pc1 = prcomp(X1, scale=TRUE, center = TRUE, rank=10)
loadings1 = pc1$rotation
scores1 = pc1$x

summary(pc1)
plot(pc1)
```

We see a bigger jump of variance explained after PC5, so we will analyze the first five principal components.

```{r}
for (i in c(1:5))
  print(sort(loadings1[,i]))
```

The above printed values are the 'ingredients' of the first five principal components. As we can see, PC3 and 4 both are primarily indicators of adult and spam categories. We will remove these variables from the dataset and re-run PCA to see if we get better variables, that differ from each other more.

```{r PCA 2}
socialmarketing2 = subset(socialmarketing1, select = -c(adult,spam))

Z2 = socialmarketing2/rowSums(socialmarketing2)
X2 = scale(Z2, center = TRUE, scale = TRUE)

pc2 = prcomp(X2, scale=TRUE, center = TRUE, rank=10)
loadings2 = pc2$rotation
scores2 = pc2$x

summary(pc2)
plot(pc2)
```

This time the last bigger dropoff in variance explained occurs after PC4, so we will use the first four principal components.

```{r}
print('PC1')
sort(loadings2[,1])
print('PC2')
sort(loadings2[,2])
print('PC3')
sort(loadings2[,3])
print('PC4')
sort(loadings2[,4])
```

Looking at the 'ingredient' set of the four principal components, this is how we classify what the components are distingushing between (the first group of people are the ones on the positive spectrum of the variable):
PC1: Internet avid vs. Fitness enthusiast
PC2: Family vs. Non-Family
PC3: Opinion sharers vs. Hobbyests -- people who share their opinion on everything vs. people who talk about their hobby's
PC4: Reverse Age (Young vs. Old)

We will now run another clustering algorithm, using the above principal components. First we need to find how many clusters there seem to be.

```{r find k with PCA}
pc3 = prcomp(X2, scale=TRUE, center = TRUE, rank=4)
scores3 = pc3$x

colnames(scores3) = c("InternetAvid", "FamilyOriented", "OpinionSharers", "Young")

#KKN and CH
set.seed(10)
N = nrow(scores3)
k_grid = c(1:30)
ss = c()
CH_grid = c()

for(k in k_grid) {
  cluster_k = kmeans(scores3, k, nstart=50)
  W = cluster_k$tot.withinss
  ss = c(ss,W)
  
  B = cluster_k$betweenss
  CH = (B/W) * ((N - k) / (k - 1))
  CH_grid = c(CH_grid, CH)
}

plot(k_grid,ss)
plot(k_grid,CH_grid)
```

Using our principal components, both approaches of finding k seem to give the same result. The elbow seems to be at k = 5, and CH is also telling us 5. Thus, we will run k-means with k = 5.


```{r K-means(5) using PCA}
set.seed(10)
kclust = kmeans(scores3, 5, nstart=50)

for(i in 1:5)
  print(sort(kclust$centers[i,] , decreasing = TRUE))

lastk = kclust$cluster
```

Analyzing the table above, the following are the five market segments we have identified:
(1) Older, family-oriented individuals
(2) Fitness enthusiasts
(3) Individuals who live their life online
(4) Tweeters -- individuals who share their opinion on everything
(5) Pop-culturists -- individuals who talk about other people's lives / what's happening and new in pop culture


```{r Plots}
final = cbind(scores3, lastk, firstk)
colnames(final) = c("InternetAvid", "FamilyOriented", "OpinionSharers", "Young", "MarketSegment", "TweetMostAbout")

final = data.frame(final)

mktCols = c('older_family_oriented', 'fitness', 'life_online', 'tweeters', 'pop_culture')
groupCols = c('AdultContent', 'TvArt', 'Dating', 'Music', 'SportsOnlineCollege', 'Beauty', 'Fitness', 'Family', 'Photo', 'News')

mkt_clusters = c()
group_clusters = c()
for (i in 1:length(final$MarketSegment))
{
  numMkt = as.numeric(final$MarketSegment[i])
  mkt_clusters = c(mkt_clusters, mktCols[numMkt])
  numGroup = as.numeric(final$TweetMostAbout[i])
  group_clusters = c(group_clusters, groupCols[numGroup])
}

final$MarketSegment = mkt_clusters
final$TweetMostAbout = group_clusters

attach(final)

ggplot(final) +
  geom_point(aes(x = InternetAvid, y = OpinionSharers, col = MarketSegment))

ggplot(final, mapping = aes(MarketSegment, fill = TweetMostAbout)) +
  geom_histogram(stat = 'count', position = position_dodge(width = 0.8)) 
```

The first graph visualizes how our identified market segments differ on the first two principal components we identified (Internet Avid users and Opinion Sharers). Visualizing the market segments on other principal components demonstrates that each segment is distinguishable from the others.

The second graph identifies what the individuals in each market segment tweet most about (which was identified in the first part of this problem). Looking at this data, our market segmentations make sense. For our fitness market, most users are tweeting most about fitness and beauty, and the largest presence of dating tweets is also found in this segment. Within our life_online segment, most individuals are sharing photos with their tweets. Next, in our older_family_oriented segment, most users are tweeting about family related things, which include sports_fandom, religion, etc. as concluded from a correlation matrix at the very beginning. Our pop_culture segment includes users who tweet about music, tv and art, and college, online gaming, and playing sports most. Lastly, our tweeters market segment tweets most about news, which is exactly who we hoped to capture in that segment.


# Author attribution

Initially, we read in all of the training data and for each author in the direcory, we read in all of their documents. We made a note of their name, in order to create the Y_train values.
We then processed all of those documents in english and combined all of the documents into one training corpus. In addition to removing numbers, punctuation, excess white-space, and making all letters lower case, we removed all SMART stopwords, since these are very common words that will be present in high frequencies in almost all documents.
After this data cleaning, we parsed the documents into a document term matrix and decided to use term-frequency asthe weighting. We also removed sparse terms to a threshold of 0.975.

```{r}
library(tm) 
library(magrittr)
library(slam)
library(proxy)

# Roll all train directories into one corpus
author_dirs_train = Sys.glob('data/ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL

Y_train = NULL

for(author in author_dirs_train) {
	author_name = substring(author, first=29)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list_train = append(file_list_train, files_to_add)
	labels_train = append(labels_train, rep(author_name, length(files_to_add)))
	
	parsedDocument = strsplit(author, "/")[[1]]
  authorName = parsedDocument[length(parsedDocument)]
  Y_train = append(Y_train, rep(authorName, length(files_to_add)))
}

readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

# Need a more clever regex to get better names here
train_docs = lapply(file_list_train, readerPlain) 
names(train_docs) = file_list_train
names(train_docs) = sub('.txt', '', names(train_docs))

# Preprocessing
my_train_corpus = Corpus(VectorSource(train_docs))
my_train_corpus = tm_map(my_train_corpus, content_transformer(tolower)) # make everything lowercase
my_train_corpus = tm_map(my_train_corpus, content_transformer(removeNumbers)) # remove numbers
my_train_corpus = tm_map(my_train_corpus, content_transformer(removePunctuation)) # remove punctuation
my_train_corpus = tm_map(my_train_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_train_corpus = tm_map(my_train_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM_train = DocumentTermMatrix(my_train_corpus)
DTM_train # some basic summary statistics

class(DTM_train)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_train[1:10,1:20])
DTM_train = removeSparseTerms(DTM_train, 0.975)
DTM_train

# Now a dense matrix
X_train = as.matrix(DTM_train)
```

We now have an X_train matrix and a Y_train vector.
We will follow the same procedure for the testing data.

```{r}
# Roll all train directories into one corpus
author_dirs_test = Sys.glob('data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL

Y_test = NULL
authors = NULL

for(author in author_dirs_test) {
	author_name = substring(author, first=29)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list_test = append(file_list_test, files_to_add)
	labels_test = append(labels_test, rep(author_name, length(files_to_add)))
	
	parsedDocument = strsplit(author, "/")[[1]]
  authorName = parsedDocument[length(parsedDocument)]
  Y_test = append(Y_test, rep(authorName, length(files_to_add)))
  authors = append(authors, authorName)
}

readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

# Need a more clever regex to get better names here
test_docs = lapply(file_list_test, readerPlain) 
names(test_docs) = file_list_test
names(test_docs) = sub('.txt', '', names(test_docs))

# Preprocessing
my_test_corpus = Corpus(VectorSource(test_docs))
my_test_corpus = tm_map(my_test_corpus, content_transformer(tolower)) # make everything lowercase
my_test_corpus = tm_map(my_test_corpus, content_transformer(removeNumbers)) # remove numbers
my_test_corpus = tm_map(my_test_corpus, content_transformer(removePunctuation)) # remove punctuation
my_test_corpus = tm_map(my_test_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_test_corpus = tm_map(my_test_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM_test = DocumentTermMatrix(my_test_corpus)
DTM_test # some basic summary statistics

class(DTM_test)  # a special kind of sparse matrix format

## You can inspect its entries...
inspect(DTM_test[1:10,1:20])
DTM_test = removeSparseTerms(DTM_test, 0.975)
DTM_test

# Now a dense matrix
X_test = as.matrix(DTM_test)
```

Now we have a properly formatted X and Y test set as well.

We decided to take out any words that were not in the test and train sets because it is likely that all authors will use new words in other articles, so it should not really have an impact on our analysis then. We did this by creating a new document term matrix for both the test and train sets, by only allowing the document term matrices to include terms that were used in the other set. So, for the test set, we only allowed the document term matrix to include words that had appeared in the training document term matrix.

```{r}
DTM_test2 = DocumentTermMatrix(my_test_corpus, control = list(dictionary=Terms(DTM_train)))
X_test2 = as.matrix(DTM_test2)

DTM_train2 = DocumentTermMatrix(my_train_corpus, control = list(dictionary=Terms(DTM_test2)))
X_train2 = as.matrix(DTM_train2)
```


Now that we have everything set up, we will find the accuracy of different models. For naive bayes, we added a smoothing parameter to the terms. We then found each terms weighting by author.
Next, we created a function that takes a test document and calculates all of the log(probabilites) of this document belonging to each author. The maximum probability is found, and the associated author is returned.
Next we looped through each document in the test set and using our above defined function we classified which author we predict this document to belong to. We kept track of how many predictions were correct.

```{r using set-2}
smooth_count = 1/nrow(X_train2)
# Create a weight matrix
author_data = X_train2[1:50,]
w_Author = colSums(author_data + smooth_count)
w_Author = w_Author/sum(w_Author)

weights = matrix(w_Author, ncol = ncol(X_train2))

for (i in 2:50)
{
  author_data = X_train2[((i*50) - 49):(i*50),]
  w_Author = colSums(author_data + smooth_count)
  w_Author = w_Author/sum(w_Author)

  weights = rbind(weights, w_Author)
}


# Create a prediction function
naiveBayesPrediction <- function(test_doc)
{
  probs = c()
  for (i in 1:50)
    probs = c(probs, sum(test_doc * log(weights[i,])))

  return (authors[which.max(probs)])
}

# Check the amount of correct predictions
yhat_test = NULL

correct = 0
for (i in 1:nrow(X_test2))
{
  test_doc = X_test2[i,]
  yhat_test = naiveBayesPrediction(test_doc)
  actual = Y_test[i]
  if (yhat_test == actual)
    correct = correct + 1
}

cat('Naive Bayes prediction accuracy:', correct/2500)
```

Our naive bayes model had a terrible prediction accuracy, just above the baseline of predicting the same author for each document. So we used a random forest model instead. We fed the training data into the model and then predicted the author for all of our testing data. 

```{r}
library(randomForest)
library(caret)
set.seed(1)

#create model using the training data
model= randomForest(y= as.factor(Y_train), x=X_train2, mtry=4, ntree=400)
#fit the model onto testing data 
model.pred<- predict(model,data= X_test2)
#simple line to calculate the accuracy
accuracy=mean(model.pred == Y_test)
#print out accuracy
accuracy
```

Our random forest model actually has a very good prediction accuracy of over 75%.

# Association Rule Mining
First step is to read in the data and process it correctly, so that it can be converted into transactions.
```{r}
library(reshape2)
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
groceries_raw = read.csv("data/groceries.txt", header=FALSE)

groceries = groceries_raw

groceries$basket = seq.int(nrow(groceries_raw))

groceries2 = melt(groceries, id.vars='basket')
groceries2 = groceries2[order(groceries2$basket),]
groceries2 = subset(groceries2, value != "")
groceries2$variable <- NULL
#groceries2 = lapply(groceries2, unique)
colnames(groceries2) = c("basket", "item")

groceries2$basket = factor(groceries2$basket)

groceries2 = split(x=groceries2$item, f=groceries2$basket)

groceries2 = lapply(groceries2, unique)

groctrans = as(groceries2, "transactions")

summary(groctrans)
```

We have 15296 transaction in which a total of 169 different items are purchased.

We will err on the side of using less restrictive rules initially, in order to not exclude any potentially interesting rules, just because they occur infrequently.
Given that the average amount of items per basket is 2.835, if all items occurred equally often, each item's support count would be around 0.017 (calculated below). This is the average support for any one item in a basket. The average support for two items or more in any basket is significantly lower. While we want to focus on rules that have a significant enough support count, this initial calculation demonstrates that our support count should be fairly low and gives us a ballpark of what it should be. Using this calculation, we decided to use a support of 0.001. While this means that the items need only show up in at least 16 of the over 15,000 transactions, this is actually more limiting than one might expect, given that there were only 4 items max allowed per transaction. Additionally, we do not want to preliminarily knock out potentially strong rules of itemsets that occur rarely. If an item is rare, for example caviar, it will have a low support count; however, there could be a strong and important rule generated by it.
Additionally, we will use a minimum confidence of 0.1. This means that for every transaction that has item A, at least one tenth of them must include item B, for there to be a rule established. While this value is chosen somewhat arbitrarily, we believe that it is a good confidence level, again given that only 4 items are allowed per transaction maximum. If there were more items allowed, we would likely increase the confidence threshold.
```{r}
2.835*15296/169/15296

grocrules = apriori(groctrans, 
	parameter=list(support=.001, confidence=.1))

inspect(grocrules)
```

Using the above mentioned thresholds, this leaves us with 548 rules.

```{r}
plot(grocrules, measure = c("support", "lift"), shading = "confidence")
```

Of these 548 rules, we can see that many of them have a small lift. However, we want rules that have a high lift, as a higher lift indicates an increased likelihood of purchasing item X when Y was bought, rather than just the general likelihood of item X being bought. These are known as complimentary products. We will look at rules with a lift of 2 or greater.

```{r}
lift2rules = subset(grocrules, subset=lift > 2)
summary(lift2rules)

plot(lift2rules, measure = c("support", "lift"), shading = "confidence")
plot(lift2rules, method='two-key plot')
```

Using a minimum lift of 2 gives us 261 rules.The first graph shows us that lift is still mostly small, and that higher lift rules have a small support, which makes intuitive sense, as the higher support there is, the lower the maximum lift possible. The second graph shows that confidence is typically higher for order 3 itemsets than order 2, but that the support for order 2 itemsets is higher. Again, this makes sense, since it is more likely for any two items to appear together than 3 items.

From these rules, we will  export the top 200 with the highest lift and analyze them more in depth.

```{r}
saveAsGraph(head(grocrules, n = 200, by = "lift"), file = "grocrules.graphml")
```

```{r}
knitr::include_graphics("Network.pdf")
```
Some sample insights:
(1) Sausage and rolls/buns (together with a few order 2 itemsets) cerate a structural hole which connects drinks with the rest of the store. So you would like to have these items at the end of the tropical fruit section to make a natural flow into the drink section.
(2) Another interesting insight from the drinks section is that fruit and vegetable juices connect the beverages community with bakery and chocolate products. So you would probably like to make a transition form beverages to the bakery/chocolate section using juices as a transition.
(3) Whipped/sour cream provide a connection from the frozen section to the rest of the items.
(4) Soft cheese and domestic eggs connect baking products to the rest of the items.

A grocery store should be designed to where the structural holes that can be discerned from the above graph are the transition items between the sections they group together. Furthermore, the largest nodes are the most important items (as measuered by betweeness centrality), meaning that they connect the largest amounts of items together. Therefore, these items should ideally be in the centers of the communities of items they represent.